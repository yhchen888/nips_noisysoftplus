\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{deng2009imagenet}
\citation{silver2016mastering}
\citation{silver2016mastering}
\citation{indiveri2009artificial}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Jug_etal_2012}
\citation{siegert1951first}
\citation{hunsberger2015spiking}
\citation{liu2016noisy}
\citation{cao2015spiking,diehl2015fast}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparisons of processing mechanisms of an artificial and a spiking neuron. (a) An artificial neuron takes numerical values of vector \textbf  {x} as input, works as a weighted summation followed by an activation function $f$. (b) Spike trains flow into a spiking neuron as input stimuli, trigger linearly summed-up PSPs through synapses with different synaptic efficacy \textbf  {w}, and post-synaptic neuron generates output spikes when membrane potential reaches some threshold.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:compare_as}{{1}{2}{Comparisons of processing mechanisms of an artificial and a spiking neuron. (a) An artificial neuron takes numerical values of vector \textbf {x} as input, works as a weighted summation followed by an activation function $f$. (b) Spike trains flow into a spiking neuron as input stimuli, trigger linearly summed-up PSPs through synapses with different synaptic efficacy \textbf {w}, and post-synaptic neuron generates output spikes when membrane potential reaches some threshold.\relax }{figure.caption.2}{}}
\citation{davison2008pynn}
\citation{davison2008pynn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}{section.2}}
\newlabel{sec:back}{{2}{3}{Background}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Noisy Softplus models the LIF response function. (a) Actual firing rates measured by simulations on an LIF neuron driven by different input currents and discrete noise levels. Bold lines show the average and the grey colour fills the range between the minimum and the maximum. (b) Noisy Softplus activates the input $x$ according to different noise levels where $k=0.16$.\relax }}{3}{figure.caption.3}}
\newlabel{fig:ns}{{2}{3}{Noisy Softplus models the LIF response function. (a) Actual firing rates measured by simulations on an LIF neuron driven by different input currents and discrete noise levels. Bold lines show the average and the grey colour fills the range between the minimum and the maximum. (b) Noisy Softplus activates the input $x$ according to different noise levels where $k=0.16$.\relax }{figure.caption.3}{}}
\newlabel{equ:nsp}{{1}{3}{Background}{equation.2.1}{}}
\newlabel{equ:distr}{{2}{3}{Background}{equation.2.2}{}}
\newlabel{equ:logist}{{3}{3}{Background}{equation.2.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Default parameter settings for the current-based LIF neurons used through this paper, for PyNN\nobreakspace  {}\cite  {davison2008pynn} simulations.\relax }}{3}{table.caption.4}}
\newlabel{tbl:pynnConfig}{{1}{3}{Default parameter settings for the current-based LIF neurons used through this paper, for PyNN~\cite {davison2008pynn} simulations.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}}
\newlabel{sec:meth}{{3}{3}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Mapping Noisy Softplus to Concrete Physical Units}{3}{subsection.3.1}}
\newlabel{sec:af_model}{{3.1}{3}{Mapping Noisy Softplus to Concrete Physical Units}{subsection.3.1}{}}
\newlabel{equ:fit}{{4}{3}{Mapping Noisy Softplus to Concrete Physical Units}{equation.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Noisy Softplus fits to the actual response firing rates of LIF neurons in concrete physical units. Recorded response firing rate of an LIF neuron driven by synaptic current with two synaptic time constants: (a) $\tau _{syn}$=10\nobreakspace  {}ms and (b) $\tau _{syn}$=10\nobreakspace  {}ms. Averaged firing rates of simulation trails are shown in bold lines, and the grey colour fills the range between the minimum to maximum of the firing rates. The thin lines are the scaled Noisy Softplus.\relax }}{4}{figure.caption.5}}
\newlabel{Fig:nsptau1}{{3}{4}{Noisy Softplus fits to the actual response firing rates of LIF neurons in concrete physical units. Recorded response firing rate of an LIF neuron driven by synaptic current with two synaptic time constants: (a) $\tau _{syn}$=10~ms and (b) $\tau _{syn}$=10~ms. Averaged firing rates of simulation trails are shown in bold lines, and the grey colour fills the range between the minimum to maximum of the firing rates. The thin lines are the scaled Noisy Softplus.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Parametric Activation Functions\nobreakspace  {}(PAFs)}{4}{subsection.3.2}}
\newlabel{fig:noisy-softplus-neuron}{{4a}{4}{Noisy Softplus\relax }{figure.caption.6}{}}
\newlabel{sub@fig:noisy-softplus-neuron}{{a}{4}{Noisy Softplus\relax }{figure.caption.6}{}}
\newlabel{fig:parametric-noisy-softplus-neuron}{{4b}{4}{Parametric Noisy Softplus\relax }{figure.caption.6}{}}
\newlabel{sub@fig:parametric-noisy-softplus-neuron}{{b}{4}{Parametric Noisy Softplus\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The PAF links the firing activity of a spiking neuron to the numerical value of ANNs.\relax }}{4}{figure.caption.6}}
\newlabel{Fig:tneuron}{{4}{4}{The PAF links the firing activity of a spiking neuron to the numerical value of ANNs.\relax }{figure.caption.6}{}}
\newlabel{equ:mi_input}{{5}{4}{Parametric Activation Functions~(PAFs)}{equation.3.5}{}}
\newlabel{equ:PAF}{{6}{4}{Parametric Activation Functions~(PAFs)}{equation.3.6}{}}
\citation{liu2016bench}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Generalised SNN Training}{5}{subsection.3.3}}
\newlabel{subsec:ns_train}{{3.3}{5}{Generalised SNN Training}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Fine Tuning}{5}{subsection.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{5}{section.4}}
\newlabel{sec:result}{{4}{5}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Neural Activity}{5}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Noisy Softplus fits to the neural response firing rate in an SNN simulation. The recorded firing rate of the same kernel convolved with 10 images in SNN simulation, comparing to the prediction of activations of Softplus, ReLU, and Noisy Softplus.\relax }}{6}{figure.caption.7}}
\newlabel{fig:af_compare}{{5}{6}{Noisy Softplus fits to the neural response firing rate in an SNN simulation. The recorded firing rate of the same kernel convolved with 10 images in SNN simulation, comparing to the prediction of activations of Softplus, ReLU, and Noisy Softplus.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Recognition Performance}{6}{subsection.4.2}}
\newlabel{subsec:result_compare}{{4.2}{6}{Recognition Performance}{subsection.4.2}{}}
\citation{Jug_etal_2012}
\citation{hunsberger2015spiking}
\citation{diehl2015fast}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparisons of Loss during training using Noisy Softplus, ReLU and Softplus activation functions. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. \relax }}{7}{figure.caption.8}}
\newlabel{Fig:loss_ns}{{6}{7}{Comparisons of Loss during training using Noisy Softplus, ReLU and Softplus activation functions. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Classification accuracy compared among trained weights of Noisy Softplus, ReLU, Softplus on DNN, SNN and fine-tuned SNN.\relax }}{7}{figure.caption.8}}
\newlabel{Fig:result_bar}{{7}{7}{Classification accuracy compared among trained weights of Noisy Softplus, ReLU, Softplus on DNN, SNN and fine-tuned SNN.\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces SNN training methods comparisons.\relax }}{7}{table.caption.9}}
\newlabel{tbl:compare}{{2}{7}{SNN training methods comparisons.\relax }{table.caption.9}{}}
\citation{stromatias2013power}
\citation{diehl2015fast}
\citation{davison2008pynn}
\citation{deng2009imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The classification accuracy of 3 trials (averaged in bold lines, grey shading shows the range between minimum to maximum) over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.\relax }}{8}{figure.caption.10}}
\newlabel{fig:ca_time}{{8}{8}{The classification accuracy of 3 trials (averaged in bold lines, grey shading shows the range between minimum to maximum) over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Power Consumption}{8}{subsection.4.3}}
\newlabel{equ:energy}{{8}{8}{Power Consumption}{equation.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and Future Work}{8}{section.5}}
\bibstyle{unsrt}
\bibdata{ref}
\bibcite{deng2009imagenet}{{1}{}{{}}{{}}}
\bibcite{silver2016mastering}{{2}{}{{}}{{}}}
\bibcite{indiveri2009artificial}{{3}{}{{}}{{}}}
\bibcite{Jug_etal_2012}{{4}{}{{}}{{}}}
\bibcite{siegert1951first}{{5}{}{{}}{{}}}
\bibcite{hunsberger2015spiking}{{6}{}{{}}{{}}}
\bibcite{liu2016noisy}{{7}{}{{}}{{}}}
\bibcite{cao2015spiking}{{8}{}{{}}{{}}}
\bibcite{diehl2015fast}{{9}{}{{}}{{}}}
\bibcite{davison2008pynn}{{10}{}{{}}{{}}}
\bibcite{liu2016bench}{{11}{}{{}}{{}}}
\bibcite{stromatias2013power}{{12}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
