\relax 
\citation{liu2016noisy}
\citation{he2015delving}
\citation{neftci2013event}
\citation{buesing2011neural}
\citation{o2016deep}
\citation{Jug_etal_2012}
\citation{Stromatias2015scalable}
\citation{furber2014spinnaker}
\@writefile{toc}{\contentsline {title}{Lecture Notes in Computer Science}{1}}
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{Authors' Instructions}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{glorot2011deep}
\citation{cao2015spiking,diehl2015fast}
\citation{srivastava2014dropout}
\citation{diehl2016conversion}
\citation{merolla2014million}
\citation{hunsberger2015spiking}
\citation{Jug_etal_2012}
\@writefile{toc}{\contentsline {section}{\numberline {2}Modelling The Activation Function}{2}}
\newlabel{sec:af_model}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural Science Background}{3}}
\newlabel{equ:LIF_V}{{1}{3}}
\newlabel{equ:consI}{{2}{3}}
\newlabel{equ:noisyI}{{3}{3}}
\newlabel{equ:LIF_V2}{{4}{3}}
\citation{rauch2003neocortical,la2008response}
\citation{siegert1951first}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Response function of the LIF neuron with noisy input currents with different standard deviations.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:physics}{{1}{4}}
\newlabel{equ:siegert}{{7}{4}}
\citation{davison2008pynn}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parameter setting for the current-based LIF neurons using PyNN.\relax }}{5}}
\newlabel{tbl:pynnConfig}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Mismatch of Siegert Function to Practice }{5}}
\newlabel{subsec:practice}{{2.2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Recorded response firing rate of a LIF neuron driven by \textit  {NoisyCurrentSource} sampled at every (a) 1\nobreakspace  {}ms and (b) 10\nobreakspace  {}ms. Averaged firing rates of 10 simulation trails are shown in bold lines, and the grey colour fills the range between the minimum to maximum of the firing rates. The analytical LIF response function, Siegert formula (Equation\nobreakspace  {}(7\hbox {})), is drawn in thin lines (shown in Figure\nobreakspace  {}1\hbox {}) to compare with the practical simulation.\relax }}{6}}
\newlabel{Fig:current}{{2}{6}}
\newlabel{equ:distr}{{8}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textit  {NoisyCurrentSource} samples noisy currents from Gauss distribution at every 1\nobreakspace  {}ms (left) and 10\nobreakspace  {}ms (right). The signals are shown in time domain in (a) and (b), and spectrum domain in (c) and (d). The autocorrelation of both current signals are shown in (e) and (f). The distribution of the discrete samples are plotted in bar chart to compare with PDF of the original Gauss distribution, shown in (g) and (h).\relax }}{7}}
\newlabel{Fig:lif_curr}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Recorded response firing rate of a LIF neuron driven by synaptic current with two synaptic time constants: (a) $\tau _{syn}$=10\nobreakspace  {}ms and (b) $\tau _{syn}$=10\nobreakspace  {}ms. Averaged firing rates of 10 simulation trails are shown in bold lines, and the grey colour fills the range between the minimum to maximum of the firing rates. The firing rates recorded driven by \textit  {NoisyCurrentSource}, are drawn in thin lines (shown in Figure\nobreakspace  {}3\hbox {}) to compare with.\relax }}{8}}
\newlabel{Fig:spike_curr}{{4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Noisy currents generated by 100 Poinsson spike trains to a LIF neuron with synaptic time constant $\tau _{syn}$=1\nobreakspace  {}ms (left) and $\tau _{syn}$=10\nobreakspace  {}ms (right). The currents are shown in time domain in (a) and (b), and spectrum domain in (c) and (d). The autocorrelation of both current signals are shown in (e) and (f). The distribution of the generated samples are plotted in bar chart to compare to the expected Gauss distribution, shown in (g) and (h).\relax }}{9}}
\newlabel{Fig:lif_pois}{{5}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Noisy Softplus}{10}}
\newlabel{equ:nsp}{{9}{10}}
\newlabel{equ:logist}{{10}{10}}
\newlabel{equ:fit}{{11}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {3}ANN-Trained SNNs}{10}}
\newlabel{sec:ann_train_snn}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Noisy Softplus fits to the response function of the LIF neuron. Noisy Softplus in (a) curve sets and (b) 3D.\relax }}{11}}
\newlabel{fig:nsp}{{6}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Equivalent Input and Output}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Noisy Softplus fits to the response firing rates of LIF neurons.\relax }}{12}}
\newlabel{Fig:nsptau1}{{7}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Artificial neuron model in ANNs. \relax }}{12}}
\newlabel{Fig:neuron}{{8}{12}}
\newlabel{equ:mi_input}{{12}{12}}
\newlabel{equ:si_input}{{13}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Artificial spiking neuron takes scaled firing rates as input, then transforms weighted sum in some activation unit to its output which can be scaled-up to the firing rate of an output spike train.\relax }}{13}}
\newlabel{Fig:sneuron}{{9}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Layered-up Network}{13}}
\newlabel{subsec:ns_train}{{3.2}{13}}
\newlabel{equ:full_act}{{14}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Transforming artificial spiking neurons to artificial neurons for SNN modelling. The combined activation links the firing activity of a spiking neuron to the numerical value of ANNs.\relax }}{14}}
\newlabel{Fig:tneuron}{{10}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fine Tuning}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{14}}
\newlabel{sec:iconipResult}{{4}{14}}
\citation{liu2016bench}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Neural Activity}{15}}
\newlabel{Fig:61}{{11a}{16}}
\newlabel{sub@Fig:61}{{a}{16}}
\newlabel{Fig:62}{{11b}{16}}
\newlabel{sub@Fig:62}{{b}{16}}
\newlabel{Fig:63}{{11c}{16}}
\newlabel{sub@Fig:63}{{c}{16}}
\newlabel{Fig:64}{{11d}{16}}
\newlabel{sub@Fig:64}{{d}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Images presented in spike trains convolve with a weight kernel. (a) The $28\times 28$ Poisson spike trains in raster plot, representing 10 digits in MNIST. (b) The firing rate of all the 784 neurons of the fourth image, digit `0', is plotted as a 2D image. (c) One out of six of the trained kernels ($5\times 5$ size) in the first convolutional layer. (d) The spike trains plotted as firing rate of the neurons in the convolved 2D map.\relax }}{16}}
\newlabel{fig:cnn}{{11}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Recognition Performance}{16}}
\newlabel{subsec:result_compare}{{4.2}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  Noisy Softplus fits to the neural response firing rate in an SNN simulation. The recorded firing rate of the same kernel convolved with 10 images shown in Figure\nobreakspace  {}11\hbox {} in SNN simulation, comparing to the prediction of activations of Softplus, ReLU, and Noisy Softplus.\relax }}{17}}
\newlabel{fig:af_compare}{{12}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Output firing rates for recognising 10 hand written digits.\relax }}{18}}
\newlabel{Fig:out}{{13}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Comparisons of Loss during training using Noisy Softplus, ReLU and Softplus activation functions. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. \relax }}{18}}
\newlabel{Fig:loss_ns}{{14}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Classification accuracy compared among trained weights of Noisy Softplus, ReLU, Softplus on DNN, SNN and fine-tuned SNN.\relax }}{19}}
\newlabel{Fig:result_bar}{{15}{19}}
\citation{diehl2015fast}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparisons of classification accuracy (in \%) of ANN-trained convolutional neural models on original DNN, NEST simulated SNN, and SNN with fine-tuned (FT) model.\relax }}{20}}
\newlabel{tbl:ns_result}{{2}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The classification accuracy of 3 trials (averaged in bold lines, grey shading shows the range between minimum to maximum) over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.\relax }}{20}}
\newlabel{fig:ca_time}{{16}{20}}
\citation{stromatias2013power}
\citation{diehl2015fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Power Consumption}{21}}
\newlabel{equ:energy}{{17}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Summary}{21}}
\citation{tensorflow2015-whitepaper}
\citation{davison2008pynn}
\citation{rokach2010ensemble}
\citation{deng2009imagenet}
\bibdata{ref}
\bibcite{buesing2011neural}{1}
\bibcite{cao2015spiking}{2}
\bibcite{davison2008pynn}{3}
\bibcite{diehl2015fast}{4}
\bibcite{diehl2016conversion}{5}
\bibcite{furber2014spinnaker}{6}
\bibcite{glorot2011deep}{7}
\bibcite{he2015delving}{8}
\bibcite{hunsberger2015spiking}{9}
\bibcite{Jug_etal_2012}{10}
\bibcite{la2008response}{11}
\bibcite{liu2016noisy}{12}
\bibcite{liu2016bench}{13}
\bibcite{merolla2014million}{14}
\bibcite{neftci2013event}{15}
\bibcite{o2016deep}{16}
\bibcite{rauch2003neocortical}{17}
\bibcite{siegert1951first}{18}
\bibcite{srivastava2014dropout}{19}
\bibcite{stromatias2013power}{20}
\bibcite{Stromatias2015scalable}{21}
\bibstyle{splncs03}
